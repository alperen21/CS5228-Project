{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFCL578S1Rde8ohnC0wkfC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# prompt: Mount drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kw8GJSuoXk_f","executionInfo":{"status":"ok","timestamp":1710661745514,"user_tz":-480,"elapsed":22328,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"c3da376b-5b00-4238-c225-d91b85ad7802"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FP85Wc_tQ5De","executionInfo":{"status":"ok","timestamp":1710662576240,"user_tz":-480,"elapsed":37,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"2a8d1d08-f1f1-48ce-cdcb-31ee3f1b6491"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import torch\n","import os\n","from transformers import pipeline\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","import re\n","from collections import Counter\n","import torch.nn as nn\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","\n","class Net(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, output_size)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        return self.fc2(x)\n","\n","class Classifier():\n","  def __init__(self):\n","    self.model = torch.load(os.path.join('/content/drive','MyDrive', 'Project', 'cluster_model.pt'))\n","    self.nlp = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa', return_all_scores=True)\n","    self.compiled_re = re.compile(r'[^a-zA-Z\\s]')\n","\n","  def preprocess_reviews(self, text):\n","      stop_words = set(stopwords.words('english'))\n","      text = self.compiled_re.sub('', text).lower()\n","      tokens = word_tokenize(text)\n","\n","      return \" \".join(tokens)\n","\n","  def average_sentiments(self, sentiments_list):\n","    sentiment_sums = {}\n","    sentiment_counts = {}\n","    for sentiments in sentiments_list:\n","        for sentiment in sentiments:\n","            label = sentiment['label']\n","            score = sentiment['score']\n","            if label in sentiment_sums:\n","                sentiment_sums[label] += score\n","                sentiment_counts[label] += 1\n","            else:\n","                sentiment_sums[label] = score\n","                sentiment_counts[label] = 1\n","    average_sentiments = {label: sentiment_sums[label] / sentiment_counts[label] for label in sentiment_sums}\n","    return average_sentiments\n","\n","  def predict(self, review_text):\n","    review_text = self.preprocess_reviews(review_text)\n","    # Split the review text into segments of up to 512 characters\n","    review_text_segments = [review_text[i:i+512] for i in range(0, len(review_text), 512)]\n","    results = []\n","\n","    # Analyze the sentiment of each text segment\n","    for segment in review_text_segments:\n","        segment = segment[:512]  # Ensure segment is not longer than 512 characters\n","        result = self.nlp(segment, return_all_scores=True)\n","        results.extend(result)\n","\n","    # Average the sentiment scores from all segments\n","    average_sentiments_list = self.average_sentiments(results)\n","\n","    # Convert the averaged sentiments into a tensor (if needed for further processing)\n","    sentiment_vector = [average_sentiments_list[key] for key in sorted(average_sentiments_list.keys())]\n","    sentiment_tensor = torch.tensor(sentiment_vector).float().unsqueeze(0)  # Add batch dimension if needed\n","\n","    # Pass the sentiment tensor through the model\n","    net_output = self.model(sentiment_tensor)\n","\n","    # Get the index of the maximum value (argmax)\n","    _, predicted_index = torch.max(net_output, dim=1)  # Assuming net_output is 2D: [batch_size, num_classes]\n","\n","    return predicted_index.item() -1 # Return the index as a Python int\n","\n","  def predict_multiple(self, review_texts):\n","    predictions = []\n","    for review_text in review_texts:\n","        prediction = self.predict(review_text)\n","        predictions.append(prediction)\n","\n","    item_counts = Counter(predictions)\n","\n","    # Find the most common item\n","    most_common_item = item_counts.most_common(1)\n","\n","    return most_common_item[0][0]"]},{"cell_type":"code","source":["classifier = Classifier()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s__2ANKdaBpJ","executionInfo":{"status":"ok","timestamp":1710662584841,"user_tz":-480,"elapsed":8616,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"6055c9bb-7840-43d5-c02e-d6bcc8058984"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at arpanghoshal/EmoRoBERTa.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["classifier.predict('this is a test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdTM8lmQaISH","executionInfo":{"status":"ok","timestamp":1710662585391,"user_tz":-480,"elapsed":562,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"9b309667-7e9f-4963-a373-dbdc0950d715"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["classifier.predict_multiple(['this is a test', 'this is another test'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbqoOHqEakZB","executionInfo":{"status":"ok","timestamp":1710662585794,"user_tz":-480,"elapsed":413,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"07f5ce6d-109b-4c98-e3b8-ac42d16e3cf6"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import pickle\n","\n","\n","# Save the classifier object to a file using pickle\n","with open(os.path.join('/content/drive','MyDrive', 'Project', 'classifier.pkl'), 'wb') as file:\n","    pickle.dump(classifier, file)\n","\n","print(\"Classifier saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSEC5ZF9bVPp","executionInfo":{"status":"ok","timestamp":1710662651988,"user_tz":-480,"elapsed":17921,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"f1fdef90-07f3-4869-aa60-4f18f1a5bf1d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n","  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Classifier saved successfully.\n"]}]},{"cell_type":"code","source":["class Recommender():\n","  def __init__(self):\n","      with open(os.path.join('/content/drive/MyDrive/', 'Project', 'cluster_rules.pkl'), 'rb') as file:\n","        self.cluster_rules = pickle.load(file)\n","\n","      with open(os.path.join('/content/drive/MyDrive/', 'Project', 'rules.pkl'), 'rb') as file:\n","        self.universal_rules = pickle.load(file)\n","\n","  def recommend_helper(self, rules, items, n_recommendations):\n","      # Filter rules with the input items as antecedents\n","      filtered_rules = rules[rules['antecedents'].apply(lambda x: x.issubset(items))]\n","\n","      # Sort rules by confidence, lift, or other metric\n","      sorted_rules = filtered_rules.sort_values(by=['confidence', 'lift'], ascending=False)\n","\n","      # Get the consequents from the rules as recommendations\n","      recommendations = sorted_rules['consequents'].apply(lambda x: list(x)).tolist()\n","\n","      # Flatten the list and remove duplicates\n","      recommendations = list(set([item for sublist in recommendations for item in sublist]))\n","\n","      # Remove input items from recommendations\n","      recommendations = [r for r in recommendations if r not in items]\n","\n","      return recommendations[:n_recommendations]\n","\n","\n","  def recommend(self, items, cluster, n_recommendations=5):\n","      \"\"\"\n","      Recommend new items based on a set of input items.\n","\n","      Parameters:\n","      - items: a set of items for which to find recommendations\n","      - rules: precomputed association rules\n","      - n_recommendations: the number of recommendations to return\n","\n","      Returns: a sorted list of recommended items\n","      \"\"\"\n","      if cluster is None:\n","        return set(self.recommend_helper(self.universal_rules, items, n_recommendations))\n","      else:\n","        return set(self.recommend_helper(self.cluster_rules[cluster], items, n_recommendations) + self.recommend_helper(self.universal_rules, items, n_recommendations))\n","\n","# Example usage:\n","items = {'bioshock'}\n","recommender = Recommender()\n","recommender.recommend(items, 1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0O9JhTbcEWl","executionInfo":{"status":"ok","timestamp":1710662859293,"user_tz":-480,"elapsed":561,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"d673b61f-6cde-45fa-a3a1-2e78369c1306"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'grand theft auto v',\n"," 'orion prelude',\n"," 'rising stormred orchestra 2 multiplayer',\n"," 'rocket league',\n"," 'torchlight ii'}"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["with open(os.path.join('/content/drive','MyDrive', 'Project', 'recommender.pkl'), 'wb') as file:\n","    pickle.dump(recommender, file)\n","\n","print(\"Recommender saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0a4FWbQcxBO","executionInfo":{"status":"ok","timestamp":1710662883160,"user_tz":-480,"elapsed":41,"user":{"displayName":"Alperen Yıldız","userId":"08245099958541946608"}},"outputId":"32e8253f-e69f-4427-e3e7-97a06db7d286"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Recommender saved successfully.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qW__L91ldA0n"},"execution_count":null,"outputs":[]}]}